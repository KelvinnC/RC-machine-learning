# TRAX. A Deep Learning Self-Driving RC Car

A deep learning system for a self driving RC car. The system collects driving data via a controller, trains a CNN to predict steering and throttle from camera images, and deploys the model for driving.

---

## Architecture

**Model**: PilotNet (NVIDIA-inspired CNN) — **252,230 parameters**

| Layer | Type | Details |
|-------|------|---------|
| 1-5 | Convolutional | 3→24→36→48→64→64 channels, ReLU activations |
| 6-8 | Fully Connected | 1152→100→50→10, Dropout (0.5) |
| Output | Dual Head | Steering [-1, 1] and Throttle [-1, 1] |

**Input**: RGB images resized to 66x200, normalized to [-1, 1]

## Dataset

- **Total frames**: 8,319 labeled images collected from manual RC car driving
- **Train / Val split**: 6,655 (80%) / 1,664 (20%)
- **Labels**: Steering pulse width and throttle pulse width (normalized)
- **Augmentation**: Random horizontal flips (with steering sign inversion), brightness jitter (HSV, 0.8-1.2x)

## Training Configuration

| Parameter | Value |
|-----------|-------|
| Framework | PyTorch 2.5.1 + CUDA 12.1 |
| GPU | NVIDIA RTX 4070 Ti SUPER |
| Optimizer | Adam (lr=0.001) |
| LR Schedule | ReduceLROnPlateau (factor=0.5, patience=3) |
| Batch Size | 128 |
| Loss | Weighted MSE (steering=1.0, throttle=0.5) |
| Precision | Mixed (AMP) |
| Early Stopping | Patience=5 |

## Training Results

Training converged in **42 epochs** (77.6 seconds total) with early stopping triggered after no improvement past epoch 37.

**Best model (Epoch 37):**

| Metric | Training | Validation |
|--------|----------|------------|
| Loss (MSE) | 0.00151 | **0.000433** |
| Steering MAE | 0.0202 | **0.0109** |
| Throttle MAE | 0.0281 | **0.0126** |

### Training Curve

The training data plot shows loss convergence over all epochs. The model rapidly reduces loss in the first 10 epochs before gradually refining through the remaining epochs. Learning rate was reduced from 0.001 to 0.0005 at epoch 41.

![Training Loss Curve](CF3%20Img/training_data.jpg)

## Evaluation Results

The following plots were generated by evaluating the best model checkpoint against the validation set.

### Scatter Plots — Predicted vs Actual

Scatter comparison of predicted steering/throttle values against ground truth. Points close to the diagonal indicate accurate predictions.

![Scatter Plots](CF3%20Img/01_scatter_plots.png)

### Time Series — Sequential Predictions

Time-series overlay of predicted and actual steering/throttle values across sequential frames. Shows the model's ability to track real driving behavior over time.

![Time Series](CF3%20Img/02_time_series.png)

### Error Distribution

Histogram of prediction errors for steering and throttle. A tight distribution centered around zero indicates consistent accuracy.

![Error Distribution](CF3%20Img/03_error_distribution.png)

### Sample Predictions

A selection of input frames with their corresponding ground truth and predicted steering/throttle values.

![Sample Predictions](CF3%20Img/04_sample_predictions.png)

### Worst Predictions

The frames where the model produced the largest prediction errors. These highlight edge cases and potential areas for data collection improvement.

![Worst Predictions](CF3%20Img/05_worst_predictions.png)

---

## Getting Started

### Prerequisites
- Windows (with WSL for training recommended) or Linux.
- Python 3.11+
- NVIDIA GPU (for training) with CUDA support.

### Installation
1.  **Create a Virtual Environment**:
    ```bash
    python -m venv venv
    source venv/bin/activate  # Linux/WSL
    # or
    .\venv\Scripts\activate   # Windows
    ```

2.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

### Usage

#### 1. Collect Data
Run the controller script on the RC car (Raspberry Pi/Jetson/PC):
```bash
cd PY
python controller-experiment.py
```
*   **Controls**:
    *   `R2`: Accelerator.
    *   `L-Stick`: Steering.
    *   `R2 (Full Press)`: Toggle Recording ON/OFF.
    *   `Square/Circle`: Adjust Steering Trim.

#### 2. Train the Model
1.  Open `pytorch_gpu_notebook.ipynb` in Jupyter Lab/Notebook.
2.  Update the `CSV` and `IMG` paths in **Section 1** to point to your collected dataset.
3.  Run all cells to train the model.
4.  The best model will be saved to the path specified in the training loop.

#### 3. Autonomous Driving (Inference)
Run a script similar to `controller.py` but replacing the PS4 controller input with model predictions:
```python
# Example pseudo-code
model = torch.jit.load("best_model.pt")
image = capture_frame()
steering, throttle = model(image)
set_pwm(steering, throttle)
```

## Hardware

| Component | Details |
|-----------|---------|
| Compute | PC / Jetson Nano / Raspberry Pi 4 |
| Camera | USB Webcam or CSI Camera |
| Controller | PS4 Wireless (Bluetooth, via evdev) |
| Actuators | RC Servo (steering) + ESC (throttle) |
| PWM Driver | PCA9685 (I2C, 50 Hz, 12-bit) |

## Model Export

The trained model is exported in multiple formats for deployment flexibility:

- **PyTorch** (`.pt`) — native checkpoint for GPU/CPU inference
- **ONNX** (`.onnx`) — cross-platform inference via onnxruntime

## Repository Structure

```
selfDriving/
├── python_scripts/          # Data collection & hardware control
│   ├── controller-experiment.py   # Main data collection (threaded)
│   └── ...
├── pytorch_gpu_notebook.ipynb   # Primary training notebook (recommended)
├── pytorch_models/          # Saved model checkpoints
├── CF3 Img/                 # Evaluation plots & training curves
├── requirements.txt         # Python dependencies
└── README.md
```
